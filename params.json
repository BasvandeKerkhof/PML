{"name":"Pml","tagline":"Practical Machine Learning Coursera","body":"---\r\ntitle: \"Practical Machine Learning Course Project\"\r\nauthor: \"Bas van de Kerkhof\"\r\ndate: \"Monday, November 16, 2015\"\r\noutput: html_document\r\n---\r\n\r\nThis report contains the documentation for the course project of the practical machine learning coursera course.\r\n\r\n#Background\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. \r\n\r\nIn this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. \r\n\r\nThe data used to train our model are available here: \r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n\r\nThe data used to test our model on are available here:\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\n#Model estimation\r\nWe will now describe how we trained our model and used it to predict the outcome based on the test data.\r\n\r\n```{r setup, include=FALSE}\r\n#Set global cache option to TRUE to reduce compiling time.\r\nknitr::opts_chunk$set(cache=TRUE)\r\n```\r\n\r\n##Preprocessing\r\nFirst we load some packages used for out model estimation. Next we set the seed for reproducibility and load the data.\r\n```{r, message=FALSE, warning=FALSE}\r\n#Load required packages\r\nlibrary(caret)\r\nlibrary(randomForest)\r\n\r\n#Set working directory correctly\r\nsetwd(\"~/Data Science/Practical Machine Learning/Course_project\")\r\n\r\n#Set seed for reproducibility\r\nset.seed(39)\r\n\r\n#Load the data\r\ntraining_data <- read.csv(\"pml-training.csv\")\r\ntest_data <- read.csv(\"pml-testing.csv\")\r\n```\r\n\r\n##Cleaning the data\r\nWhen we look at the top part of the data we see that there are some columns contain a lot of NA-values. We want to remove these columns.\r\n\r\n```{r}\r\n#Calculate percentage of NA values per column\r\nnof_rows <- nrow(training_data)\r\nperc_NA <- colSums(is.na(training_data))/nof_rows\r\n\r\n#Keep the columns which have less than 10% NA values\r\nkeep_cols <- perc_NA < 0.1\r\ntraining_data <- training_data[,keep_cols]\r\n```\r\n\r\nWe also delete variables which have near zero variance since they will not contribute to the model estimation.\r\n\r\n```{r}\r\n#Find near zero variance variables and delete them\r\nnzv_train <- nearZeroVar(training_data, saveMetrics=TRUE)\r\ntraining_data <- training_data[, nzv_train$nzv==FALSE]\r\n```\r\n\r\nLets observe the remaining training data to see if further processing is needed.\r\n```{r}\r\n#Check for complete cases in remainder of data\r\nsum(!complete.cases(training_data))\r\nhead(training_data)\r\n```\r\n\r\nWe see that all rows are complete, which is good. But we also see that the first six columns do not contain any usefull information for the model estimation. Therefore we remove these columns.\r\nThe remaining training data is partitioned into a training set and a cross validation set.\r\n\r\n#Estimate the model\r\n```{r}\r\n#Delete the first six columns, since they do not contain valuable information\r\ntraining_data <- training_data[,-(1:6)]\r\n\r\n#Partition the data into training and cross validation set\r\ninTrain <- createDataPartition(training_data$classe, p=0.70, list=F)\r\ntrain_data <- training_data[inTrain, ]\r\nCV_data <- training_data[-inTrain, ]\r\n```\r\n\r\nNow we have got a cleaned training dataset on which we can estimate our model. We will use a random forest to estimate our model since this is one of the most accurate algorithms out ther. We use the randomForest function from the randomForest package since this function is much faster than the train function from the caret package.\r\nWe cross validate the estimated model using our partition of cross validation data.\r\n\r\n```{r}\r\n#Determine the model using a random forest\r\nmodel <- randomForest(classe ~. , data=train_data)\r\n\r\n#Predict our model using the cross validation data and show outcome\r\nCV_prediction <- predict(model, CV_data)\r\nconfusionMatrix(CV_data$classe, CV_prediction)\r\n```\r\n\r\nWhen we look at the results of the confusion matrix we see that we get an accuracy of 99.52%. An hence we expect the out of sample error to be 0.48% (=1-accuracy).\r\n\r\nFinally we use our model for prediction on the test data. Of course we do not pre process the test data since otherwise we would be manipulating the test results. We also convert the outcome to the desired format for automated grading.\r\n\r\n#Predict the outcome\r\n```{r}\r\n#Predict our model using the test_data (which of course we did not preprocess)\r\nprediction <- predict(model, test_data)\r\n\r\n#Write files to desired output for automatic grading\r\npml_write_files = function(x){\r\n    n = length(x)\r\n    for(i in 1:n){\r\n        filename = paste0(\"problem_id_\",i,\".txt\")\r\n        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n    }\r\n}\r\n\r\npml_write_files(prediction)\r\n```\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}